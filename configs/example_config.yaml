train_on_completions: true 
devices: [0, 1]
trainer_config:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  num_train_epochs: 1
  learning_rate: 4e-3
  warmup_steps: 5
  fp16: false
  bf16: true
  logging_steps: 1
  optim: adamw_8bit
  lr_scheduler_type: cosine
  seed: 3407
  save_strategy: steps
  save_steps: 200
  per_device_eval_batch_size: 1
  # eval_accumulation_steps: 1
  # eval_strategy: steps
  # batch_eval_metrics: true
  run_name: run1
peft_config:
  r: 16
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj","gate_proj", "up_proj", "down_proj", "lm_head"]
  lora_alpha: 32
  lora_dropout:  0
  bias: none
  use_gradient_checkpointing: unsloth
  random_state: 3407
  use_rslora: true
pretrained_model_config:
  model_name: unsloth/llama-3-8b-Instruct-bnb-4bit
  max_seq_length: 8192
  load_in_4bit: true
dataset_config:
  dataset_module_name: two_tasks_instruct_dataset_llama
  validation_data_path: /path/to/validation/data.json
  training_data_path: /path/to/training/data.json
  instruction_part: "<|start_header_id|>user<|end_header_id|>\n\n"
  response_part: "<|start_header_id|>assistant<|end_header_id|>\n\n"